{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\vdinh\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\vdinh\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vdinh\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vdinh\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vdinh\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vdinh\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "# install\n",
    "################################\n",
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vdinh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vdinh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vdinh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################\n",
    "# imports\n",
    "################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "from tqdm import tqdm  #\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Download necessary NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# test variable\n",
    "################################\n",
    "\n",
    "test = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# load in the world new data\n",
    "################################\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "if test:\n",
    "    print('current_dir: ' + str(current_dir) + '\\n')\n",
    "\n",
    "#csv_path = current_dir + \"\\Top_25_World_News_2018_2023\\WorldNewsData.csv\"\n",
    "csv_path = r\"{}\\Top_25_World_News_2018_2023\\WorldNewsDataV2.csv\".format(current_dir)\n",
    "\n",
    "if test:\n",
    "    print('csv_path: ' + str(csv_path) + '\\n')\n",
    "\n",
    "# convert csv to pandas dataframe\n",
    "news_data = pd.read_csv(csv_path)\n",
    "\n",
    "if test:\n",
    "    print('len(news_data)    :' + str(len(news_data))    + '\\n')\n",
    "    print('news_data.iloc[0] :' + str(news_data.iloc[0]) + '\\n')\n",
    "    print('news_data.head()  :' + str(news_data.head())  + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# load in the stock data\n",
    "################################\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "dirs = [\n",
    "        os.path.join(os.getcwd(), \"Stock_Market_Data_NASDAQ_NYS_S&P500\", \"stock_market_data\", \"forbes2000\"),\n",
    "        os.path.join(os.getcwd(), \"Stock_Market_Data_NASDAQ_NYS_S&P500\", \"stock_market_data\", \"nasdaq\"),\n",
    "        os.path.join(os.getcwd(), \"Stock_Market_Data_NASDAQ_NYS_S&P500\", \"stock_market_data\", \"nyse\"),\n",
    "        os.path.join(os.getcwd(), \"Stock_Market_Data_NASDAQ_NYS_S&P500\", \"stock_market_data\", \"sp500\"),\n",
    "       ]\n",
    "\n",
    "#csv_files = [os.path.join(csv_dir, file) for file in os.listdir(csv_dir)]\n",
    "\n",
    "if test:\n",
    "    print('current_dir: ' + str(current_dir) + '\\n')\n",
    "    #print('csv_files  : ' + str(csv_files) + '\\n')\n",
    "\n",
    "# save list to handle duplicate stocks\n",
    "processed_files = set()\n",
    "stock_data      = []\n",
    "\n",
    "# convert csv to pandas dataframe then combine\n",
    "for dir in dirs:\n",
    "    csv = [os.path.join(dir, file) for file in os.listdir(dir)]\n",
    "    for file in csv:\n",
    "        # take only first part of file name (leave out csv)\n",
    "        file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "        # if duplicate, continue and skip\n",
    "        if file_name in processed_files:\n",
    "            continue\n",
    "        # read in file and append\n",
    "        df = pd.read_csv(file)\n",
    "        df['Stocks'] = file_name\n",
    "        stock_data.append(df)\n",
    "        processed_files.add(file_name)\n",
    "\n",
    "stock_data = pd.concat(stock_data, ignore_index=True)\n",
    "\n",
    "if test:\n",
    "    print('len(stock_data)    :' + str(len(stock_data))    + '\\n')\n",
    "    print('stock_data.iloc[0] :' + str(stock_data.iloc[0]) + '\\n')\n",
    "    print('stock_data.iloc[1] :' + str(stock_data.iloc[1]) + '\\n')\n",
    "    print('stock_data.head()  :' + str(stock_data.head()) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# remove dates older than May 01, 2018 for stock data\n",
    "#######################################################\n",
    "\n",
    "# stock_data.iloc[1] :Date              19-11-1999\n",
    "stock_data['Date'] = pd.to_datetime(stock_data['Date'], format='%d-%m-%Y')\n",
    "filt_new_data = stock_data[stock_data['Date'] >= '2018-05-01']\n",
    "filt_new_data = filt_new_data.reset_index(drop=True)\n",
    "\n",
    "if test:\n",
    "    print('len(filt_new_data)    :' + str(len(filt_new_data))    + '\\n')\n",
    "    print('filt_new_data.head()  :' + str(filt_new_data.head())  + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert news data format to stocks data\n",
    "\n",
    "#news_data.iloc[0] :Date                                          May 01, 2018\n",
    "news_data2 = news_data.copy()\n",
    "#news_data2['Date'] = pd.to_datetime(news_data2['Date'], infer_datetime_format=True, errors='coerce')\n",
    "#news_data2['Date Date'] = pd.to_datetime(news_data2['Date']).dt.strftime('%d-%m-%y')\n",
    "news_data2['Date'] = pd.to_datetime(news_data2['Date'], format='%b %d, %Y', errors='coerce').dt.strftime('%d-%m-%y')\n",
    "\n",
    "if test:\n",
    "    print('len(news_data2)      :' + str(len(news_data2))      + '\\n')\n",
    "    print('news_data2.iloc[50] :' + str(news_data2.iloc[50])   + '\\n')\n",
    "    print('news_data2.head()    :' + str(news_data2.head())    + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data2.to_csv('updated_news_data.csv')\n",
    "filt_new_data.to_csv('updated_stock_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install deep-translator langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "file_path = \"updated_news_data.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "columns_to_translate = df.columns[2:27]\n",
    "\n",
    "def translate_text(text):\n",
    "    try:\n",
    "        if pd.notna(text):  # Check if the text is not NaN\n",
    "            lang = detect(text)\n",
    "            if lang != \"en\":  # Translate only if not English\n",
    "                return GoogleTranslator(source=lang, target=\"en\").translate(text)\n",
    "        return text  \n",
    "    except:\n",
    "        return text  \n",
    "for col in tqdm(columns_to_translate, desc=\"Translating Columns\"):\n",
    "    df[col] = df[col].astype(str).apply(translate_text)\n",
    "\n",
    "translated_file_path = \"translated_news_data.csv\"\n",
    "df.to_csv(translated_file_path, index=False)\n",
    "\n",
    "print(f\"Translation complete. Saved to {translated_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next part, human eyes are needed in order to translate what is missed in the previous step.\n",
    "# The new .csv file is called human_translated_news_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0      Date                                               Top1  \\\n",
      "0           0  1-May-18    north korea open sky south korean medium report   \n",
      "1           1  2-May-18  got fear trump threatens declassify fbi justic...   \n",
      "2           2  3-May-18  hollywood isnt simply churning crummy remake n...   \n",
      "3           3  4-May-18  donald trump say london hospital like war zone...   \n",
      "4           4  5-May-18  missing teen mom implores canada take note u s...   \n",
      "\n",
      "                                                Top2  \\\n",
      "0             mueller probe aint ending anytime soon   \n",
      "1  best garage heater review incuding electric ga...   \n",
      "2  iran foreign minister u consistently violated ...   \n",
      "3  german seat un security council likely israel ...   \n",
      "4      william kate release new picture prince louis   \n",
      "\n",
      "                                                Top3  \\\n",
      "0  brand new 2018renault alpine a110startup amp l...   \n",
      "1  apple fueled everyones biggest fear company go...   \n",
      "2  rihanna cover june 2018 problem vogue journal ...   \n",
      "3      gang used drone swarm thwart fbi hostage raid   \n",
      "4  meghans brother sends prince harry secret lett...   \n",
      "\n",
      "                                                Top4  \\\n",
      "0  story midwestern innocent let loose big city t...   \n",
      "1  google accused using gdpr impose unfair term p...   \n",
      "2  u soldier secretly fighting saudi arabia war y...   \n",
      "3                 many carbs eat per day lose weight   \n",
      "4  australia free whooping cough vaccine offered ...   \n",
      "\n",
      "                                                Top5  \\\n",
      "0  mattis temper optimistic outlook afghanistan t...   \n",
      "1  man died accidentally climbing behind woman wa...   \n",
      "2  canadian mp bob saroya took trip china paid ch...   \n",
      "3                russian troll farm go muellers file   \n",
      "4  shock thawalaskan sea ice took steep unprecede...   \n",
      "\n",
      "                                                Top6  \\\n",
      "0          facebook taking tinder new dating feature   \n",
      "1  coveted palme dor trophy awarded starstudded c...   \n",
      "2  2017 best black friday cyber monday mattress d...   \n",
      "3     israeli soldier singing persian iranian people   \n",
      "4        giuliani trump committed regime change iran   \n",
      "\n",
      "                                                Top7  \\\n",
      "0  oecd chief say greece deserves debt relief eno...   \n",
      "1       border illegal tucker carlson v dumb liberal   \n",
      "2  nbc abc say trump lawyer michael cohen wiretap...   \n",
      "3  labor day 2017 sale memory foam mattress compared   \n",
      "4  pakistan polio eradication team visit doortodo...   \n",
      "\n",
      "                                                Top8  ...  \\\n",
      "0                     facebook launch dating service  ...   \n",
      "1       mark zuckerberg say take 3 year fix facebook  ...   \n",
      "2  south korean janitor may keep gold bar found a...  ...   \n",
      "3             chinese city turned 16000 bus electric  ...   \n",
      "4  aide donald trump u president hired israeli pr...  ...   \n",
      "\n",
      "                                               Top16  \\\n",
      "0                                iran nuclear threat   \n",
      "1   palestinian said set withdraw recognition israel   \n",
      "2  world last male northern white rhino died marc...   \n",
      "3  2013 cadillac ct luxury walkaround start tour ...   \n",
      "4  ontario pc chief doug ford say former party ma...   \n",
      "\n",
      "                                               Top17  \\\n",
      "0  jean binta breeze performance dis poem wordz a...   \n",
      "1   half australian business got tax cut banked cash   \n",
      "2  uruguay soybean crop forecasted drop 43 devast...   \n",
      "3  uk local election 4000 people turned away cast...   \n",
      "4  white supremacist patrick little top challenge...   \n",
      "\n",
      "                                               Top18  \\\n",
      "0  migrant say accidentally raped belgian woman w...   \n",
      "1                             idea make college work   \n",
      "2          working nuclear reactor planet plan waste   \n",
      "3  nico tortorella speaks onstage throughout risi...   \n",
      "4  trump stop granting temporary protected status...   \n",
      "\n",
      "                                               Top19  \\\n",
      "0         email newsletter curated marketing content   \n",
      "1  house republican nominate donald trump nobel p...   \n",
      "2  marketing customer obsession imperative c3 201...   \n",
      "3          increase sale encouraging mobile spending   \n",
      "4  revealed trump team hired spy firm dirty ops i...   \n",
      "\n",
      "                                               Top20  \\\n",
      "0  mueller asks 2 month flynn sentencing suggesti...   \n",
      "1  uk may face local election loss key brexit tes...   \n",
      "2  extrump aide michael caputo warns mueller russ...   \n",
      "3                    china schooling trump art delay   \n",
      "4           simple way improve billion life eyeglass   \n",
      "\n",
      "                                               Top21  \\\n",
      "0              mueller asks 2 month flynn sentencing   \n",
      "1             whynter icm15ls ice cream maker review   \n",
      "2  11yearold girl autism wouldnt talk class sings...   \n",
      "3                                 abi ofarim ist tot   \n",
      "4  dutch cop request removal holocaust memorial s...   \n",
      "\n",
      "                                               Top22  \\\n",
      "0  canada tack carbon price least 11 cent liter g...   \n",
      "1  abbas say jew behavior antisemitism caused hol...   \n",
      "2  richard corkeryny day information getty photo ...   \n",
      "3                             8 best snow boot woman   \n",
      "4  montreal couple agree submit peace bond condit...   \n",
      "\n",
      "                                               Top23  \\\n",
      "0              kanye west class war one reason trump   \n",
      "1  house republican nominate trump nobel peace prize   \n",
      "2                                   make brand stand   \n",
      "3  kerry quietly seeking salvage iran deal helped...   \n",
      "4  air france bos resigns 55 percent employee tur...   \n",
      "\n",
      "                                               Top24  \\\n",
      "0  michael stewartwireimage kylie jenner simply r...   \n",
      "1  tyson food largest u meat processor invested i...   \n",
      "2     uk drone syria using controversial vacuum bomb   \n",
      "3  trump syria aid freeze hit white helmet rescue...   \n",
      "4  air france bos resigns 55 percent 46771 air fr...   \n",
      "\n",
      "                                               Top25  \n",
      "0  first time health ministry regulate descriptio...  \n",
      "1  question linger melania trump scored einstein ...  \n",
      "2  high fruit diet could help woman conceive stud...  \n",
      "3            u slam turkey population transfer afrin  \n",
      "4  awardwinning filmmaker carlos carvalho dy gira...  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('human_translated_news_data.csv')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def normalize_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        normalized_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "        return ' '.join(normalized_tokens)\n",
    "    else:\n",
    "        # Return empty string for non-string data\n",
    "        return ''\n",
    "\n",
    "# Normalize each headline in Top1 to Top25 columns\n",
    "for col in df.columns[2:]:  # Skip the first two columns (Unnamed and Date)\n",
    "    df[col] = df[col].apply(normalize_text)\n",
    "\n",
    "df.to_csv('normalized_headlines.csv', index=False)\n",
    "print(df.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
